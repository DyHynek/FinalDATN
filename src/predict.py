import serial
import threading
import time
import csv
from datetime import datetime
import pytz
import pandas as pd
import numpy as np
import pywt
import joblib
from scipy.signal import find_peaks
from sklearn.feature_selection import mutual_info_regression
from numpy.linalg import norm
from sklearn.neighbors import NearestNeighbors
from scipy.stats import kurtosis



SERIAL_PORT = 'COM3'
SERIAL_BAUD = 115200
CSV_FILE_NAME = "data.csv"

TIME_FEATURE_FILE_NAME = "heart_rate_results.csv"
WAVELET_FEATURE_FILE_NAME = "wavelet_results.csv"
NONLINEAR_FEATURE_FILE_NAME = "nonlinear_results.csv"
FREQUENCY_FEATURE_FILE_NAME = "fft_results.csv"

MODEL_TIME = "model_time_domain.pkl"
MODEL_NONLINEAR = "model_nonlinear.pkl"
MODEL_WAVELET = "model_wavelet.pkl"
MODEL_FREQUENCY = "model_frequency.pkl"

OUTPUT_FILE = "final_prediction.csv"

acc_time = 0.9485
acc_wavelet = 0.6826
acc_nonlinear = 0.8796
acc_fourier = 0.6566

# ƒê·ªãnh nghƒ©a m√∫i gi·ªù Vi·ªát Nam
vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')
def get_vietnam_time():
    return datetime.now(vn_tz).strftime("%Y-%m-%d %H:%M:%S")

# Ghi d·ªØ li·ªáu t·ª´ Serial v√†o CSV
def serial_reader():
    while True:
        try:
            ser = serial.Serial(SERIAL_PORT, SERIAL_BAUD)
        except serial.SerialException as e:
            print(f"L·ªói k·∫øt n·ªëi Serial: {e}")
            return

        with open(CSV_FILE_NAME, "w", newline="") as csv_file:
            csv_writer = csv.writer(csv_file)
            csv_writer.writerow(["Time (real)", "IR Value raw", "IR Value filtered", "Time (s)"])

            print("üü¢ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu (b·ªè 5 gi√¢y ƒë·∫ßu, ghi 60 gi√¢y / ngh·ªâ 10 gi√¢y)... Nh·∫•n Ctrl+C ƒë·ªÉ tho√°t.")
            try:
                total_start = time.time()
                print("‚è≥ B·ªè qua 5 gi√¢y ƒë·∫ßu ƒë·ªÉ ·ªïn ƒë·ªãnh t√≠n hi·ªáu...")
                
                # ƒê·ª£i 5 gi√¢y ƒë·∫ßu kh√¥ng ghi d·ªØ li·ªáu
                while time.time() - total_start < 5:
                    if ser.in_waiting:
                        ser.readline()  # ƒê·ªçc b·ªè ƒë·ªÉ kh√¥ng backlog

                print("üì° B·∫Øt ƒë·∫ßu ghi d·ªØ li·ªáu trong 60 gi√¢y...")
                record_start = time.time()
                while time.time() - record_start < 60:
                    if ser.in_waiting:
                        line = ser.readline().decode('utf-8').strip()
                        try:
                            ir_raw, ir_filtered = map(int, line.split(','))
                            if ir_raw < 50000:
                                print("ƒê·∫∑t tay v√†o c·∫£m bi·∫øn!")
                        except ValueError:
                            continue
                        elapsed_time = time.time() - record_start
                        csv_writer.writerow([get_vietnam_time(), ir_raw, ir_filtered, elapsed_time])
                        csv_file.flush()

                print("‚è∏Ô∏è Ngh·ªâ 10 gi√¢y...")
                time.sleep(10)

            except KeyboardInterrupt:
                print("‚õî D·ª´ng ghi d·ªØ li·ªáu.")
            finally:
                ser.close()



# Ph√¢n t√≠ch d·ªØ li·ªáu sau 60s v√† l·∫∑p l·∫°i m·ªói 10s
def feature_time_extractor():
    while True:
        print("‚è≥ [Time] ƒêang ƒë·ª£i 60 gi√¢y ƒë·ªÉ thu ƒë·ªß d·ªØ li·ªáu...")
        time.sleep(65)  # ƒê·ª£i 60 gi√¢y cho qu√° tr√¨nh ƒëo

        try:
            df = pd.read_csv(CSV_FILE_NAME)

            # Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt
            if {"Time (s)", "IR Value filtered"}.issubset(df.columns):
                # Lo·∫°i b·ªè tr√πng v√† s·∫Øp x·∫øp
                df = df.drop_duplicates(subset=["Time (s)"]).sort_values(by=["Time (s)"])
                time_arr = np.array(df["Time (s)"])
                ir_signal = np.array(df["IR Value filtered"])
                
                # T√¨m ƒë·ªânh v√† ƒë√°y
                dt = np.mean(np.diff(time_arr))  # sampling interval
                peaks, _ = find_peaks(ir_signal, height=np.mean(ir_signal), distance=0.5 / dt)
                valleys, _ = find_peaks(-ir_signal, height=-np.mean(ir_signal), distance=0.3 / dt)

                rising_times = []
                for v in valleys:
                    future_peaks = peaks[peaks > v]
                    if len(future_peaks) == 0:
                        continue
                    p = future_peaks[0]
                    rising_times.append(time_arr[p] - time_arr[v])

                # T√≠nh ƒë·∫∑c tr∆∞ng
                if len(peaks) >= 2:
                    peak_times = time_arr[peaks]
                    intervals = np.diff(peak_times)
                    avg_interval = np.mean(intervals)
                    std_interval = np.std(intervals)
                else:
                    avg_interval = std_interval = np.nan

                if len(peaks) > 0 and len(valleys) > 0:
                    amplitude = np.mean(ir_signal[peaks]) - np.mean(ir_signal[valleys])
                else:
                    amplitude = np.nan

                avg_rising_time = np.mean(rising_times) if rising_times else np.nan
                num_beats = len(peaks)
                bpm = (num_beats / 60) * 60  # do ƒë√£ ƒëo ƒë√∫ng 60s

                # Ghi k·∫øt qu·∫£
        
                result = pd.DataFrame([[bpm, avg_interval, std_interval, amplitude, avg_rising_time]],
                                      columns=["BPM", "AVG Interval (s)", "STD Interval (s)", "Amplitude", "Time V2P (s)"])
                result.to_csv(TIME_FEATURE_FILE_NAME, index=False)
                print(f"‚úÖ ƒê√£ l∆∞u c√°c ƒë·∫∑c tr∆∞ng v√†o '{TIME_FEATURE_FILE_NAME}'")
                time.sleep(10)

        except Exception as e:
            print(f"‚ùå L·ªói x·ª≠ l√Ω feature: {e}")

def fourier_feature_extractor():
    while True:
        print("‚è≥ [Frequency] ƒêang ƒë·ª£i 60 gi√¢y tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu t√≠nh ƒë·∫∑c tr∆∞ng bi·∫øn ƒë·ªïi frequency...")
        time.sleep(65)

        try:
            df = pd.read_csv(CSV_FILE_NAME)
            if {"Time (s)", "IR Value filtered"}.issubset(df.columns):
                df = df.drop_duplicates(subset=["Time (s)"]).sort_values(by=["Time (s)"])
                time_arr = np.array(df["Time (s)"])
                ir_signal = np.array(df["IR Value filtered"])
                end_time = time_arr[-1]
                start_time = end_time - 10

                mask = (time_arr >= start_time) & (time_arr <= end_time)
                time_window = time_arr[mask]
                ir_window = ir_signal[mask]

                if len(time_window) < 2:
                    print("‚ö†Ô∏è [Frequency] D·ªØ li·ªáu trong 10 gi√¢y cu·ªëi kh√¥ng ƒë·ªß ƒë·ªÉ t√≠nh FFT.")
                    continue

                # FFT
                dt = np.mean(np.diff(time_window))
                N = len(ir_window)
                freqs = np.fft.rfftfreq(N, d=dt)
                fft_result = np.fft.rfft(ir_window)

                amplitude = np.abs(fft_result)
                phase = np.angle(fft_result)
                power = amplitude ** 2

                # T√¨m peaks & troughs pha
                peaks_all, _ = find_peaks(phase)
                peaks = peaks_all[phase[peaks_all] > 1]

                troughs_all, _ = find_peaks(-phase)
                troughs = troughs_all[phase[troughs_all] < -1]

                peak_phases = phase[peaks]
                trough_phases = phase[troughs]

                peak_mean = np.mean(peak_phases) if len(peak_phases) > 0 else 0
                peak_std = np.std(peak_phases) if len(peak_phases) > 0 else 0
                trough_mean = np.mean(trough_phases) if len(trough_phases) > 0 else 0
                trough_std = np.std(trough_phases) if len(trough_phases) > 0 else 0

                max_amplitude = np.max(amplitude)
                fma = freqs[np.argmax(amplitude)]
                total_power = np.sum(power)

                output = pd.DataFrame([[
                    peak_mean, peak_std,
                    trough_mean, trough_std, fma, total_power
                ]], columns=[
                    "AVG Peak", "STD Peak",
                    "AVG Trough", "STD Trough", "FMA", "Total Power"
                ])

                
                output.to_csv(FREQUENCY_FEATURE_FILE_NAME, index=False)

                print(f"‚úÖ [Frequency] ƒê√£ t√≠nh ƒë·∫∑c tr∆∞ng v√† l∆∞u v√†o '{FREQUENCY_FEATURE_FILE_NAME}'")
                time.sleep(10)
            else:
                print("‚ö†Ô∏è [Frequency] File kh√¥ng ch·ª©a c√°c c·ªôt c·∫ßn thi·∫øt: 'Time (s)', 'IR Value filtered', 'Label'")
        except Exception as e:
            print(f"‚ùå [Frequency] L·ªói x·ª≠ l√Ω Frequency: {e}")


def wavelet_feature_extractor():
    
    while True:
        print("‚è≥ [Wavelet] ƒêang ƒë·ª£i 60 gi√¢y tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu t√≠nh ƒë·∫∑c tr∆∞ng bi·∫øn ƒë·ªïi wavelet...")
        time.sleep(65)

        
        try:
            df = pd.read_csv(CSV_FILE_NAME)

            if {"Time (s)", "IR Value filtered"}.issubset(df.columns):
                df = df.drop_duplicates(subset=["Time (s)"]).sort_values(by=["Time (s)"])
                time_arr = np.array(df["Time (s)"])
                ir_signal = np.array(df["IR Value filtered"])
                result = []
                
           

                
                # Bi·∫øn ƒë·ªïi wavelet m·ª©c 4 v·ªõi coif5
                coeffs = pywt.wavedec(ir_signal, wavelet='coif5', level=4)
                A4, D4, D3, D2, D1 = coeffs  # Gi·∫£i n√©n h·ªá s·ªë (ng∆∞·ª£c l·∫°i v·ªõi th·ª© t·ª± tr·∫£ v·ªÅ)

                # T√≠nh kurtosis
                kurt_D1 = kurtosis(D1)
                kurt_D2 = kurtosis(D2)
                kurt_D3 = kurtosis(D3)
                kurt_A4 = kurtosis(A4)

                result.append([kurt_D1, kurt_D2, kurt_D3, kurt_A4])
                result_df = pd.DataFrame(result, columns=["Kurtosis D1", "Kurtosis D2", "Kurtosis D3", "Kurtosis A4"])
                result_df.to_csv(WAVELET_FEATURE_FILE_NAME, index=False)
                print("‚úÖ ƒê√£ l∆∞u c√°c ƒë·∫∑c tr∆∞ng v√†o 'wavelet_results.csv'")
                time.sleep(10)
        except Exception as e:
            print(f"‚ùå [Wavelet] L·ªói x·ª≠ l√Ω Wavelet: {e}")


def calculate_fnn(signal, delay, max_dim, Rtol=10.0, Atol=None):
    if Atol is None:
        Atol = 2.0 * np.std(signal)

    N = len(signal)
    fnn_percentages = []

    for d in range(1, max_dim + 1):
        M = N - (d + 1) * delay
        if M <= 0:
            break

        embedded_d = np.array([signal[i:i + d * delay:delay] for i in range(M)])
        embedded_d1 = np.array([signal[i:i + (d + 1) * delay:delay] for i in range(M)])

        false_nearest = 0
        for i in range(M):
            dists = np.linalg.norm(embedded_d - embedded_d[i], axis=1)
            dists[i] = np.inf 
            nearest_idx = np.argmin(dists)

            dist_d = dists[nearest_idx]
            dist_d1 = np.linalg.norm(embedded_d1[i] - embedded_d1[nearest_idx])

            if dist_d == 0:
                continue

            if dist_d1 / dist_d > Rtol or abs(signal[i + d * delay] - signal[nearest_idx + d * delay]) > Atol:
                false_nearest += 1

        fnn_percentages.append(false_nearest / M * 100)
    return fnn_percentages

def estimate_delay_ami(signal, max_lag):
    mi_values = []
    for lag in range(1, max_lag + 1):
        X = signal[:-lag].reshape(-1,1)
        Y = signal[lag:]
        mi = mutual_info_regression(X, Y, discrete_features=False)
        mi_values.append(mi[0])
    optimal_lag = np.argmin(mi_values) + 1 if mi_values else 1
    ami_value = mi_values[optimal_lag - 1]
    return optimal_lag, ami_value

def plot_fnn(fnn_vals):
    # T√¨m minimum c·ª•c b·ªô ƒë·∫ßu ti√™n n·∫øu c√≥
    for i in range(1, len(fnn_vals) - 1):
        if fnn_vals[i] < fnn_vals[i - 1] and fnn_vals[i] < fnn_vals[i + 1]:
            return fnn_vals[i]
    # fallback: ch·ªçn gi√° tr·ªã nh·ªè h∆°n 1% ƒë·∫ßu ti√™n, n·∫øu c√≥
    for i, val in enumerate(fnn_vals):
        if val < 1:
            return val
    return fnn_vals[0]

def reconstruct_phase_space(signal, delay, embedding_dimension):
    n_points = len(signal) - (embedding_dimension - 1) * delay
    return np.array([signal[i:i + embedding_dimension * delay:delay] for i in range(n_points)])

def calculate_lyapunov_exponent(phase_space, k=10):
    nbrs = NearestNeighbors(n_neighbors=2).fit(phase_space)
    distances, indices = nbrs.kneighbors(phase_space)
    divergence = []
    for i in range(1, len(phase_space)):
        d0 = distances[i, 1]
        if i + k < len(phase_space) and indices[i, 1] + k < len(phase_space):
            d1 = norm(phase_space[i + k] - phase_space[indices[i, 1] + k])
            if d0 > 0 and d1 > 0:
                divergence.append(np.log(d1 / d0))
    return np.mean(divergence) / k if divergence else 0

def calculate_attractor_reconstruction_error(phase_space, delay):
    N = len(phase_space)
    if N < 2:
        return np.nan
    indices = NearestNeighbors(n_neighbors=2).fit(phase_space).kneighbors(phase_space, return_distance=False)
    errors = []
    for i in range(N - delay):
        neighbor_idx = indices[i, 1]
        if neighbor_idx + delay < N:
            original_next = phase_space[i + delay]
            predicted_next = phase_space[neighbor_idx + delay]
            errors.append(norm(original_next - predicted_next))
    return np.mean(errors) if errors else np.nan

def calculate_dfa(signal, scales=None):
    if scales is None:
        scales = [10, 20, 40, 80, 100]
    flucts = []
    for scale in scales:
        segments = len(signal) // scale
        reshaped = np.reshape(signal[:segments * scale], (segments, scale))
        F = []
        for segment in reshaped:
            x = np.arange(scale)
            trend = np.polyfit(x, segment, 1)
            detrended = segment - np.polyval(trend, x)
            F.append(np.sqrt(np.mean(detrended ** 2)))
        flucts.append(np.mean(F))
    log_scales = np.log(scales)
    log_flucts = np.log(flucts)
    coeffs = np.polyfit(log_scales, log_flucts, 1)
    return coeffs[0] 

def run_nonlinear_analysis():
    
    while True:
        print("‚è≥ [Nonlinear] ƒêang ƒë·ª£i 60 gi√¢y tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu t√≠nh ƒë·∫∑c tr∆∞ng phi tuy·∫øn...")
        time.sleep(65)
        
        try:
            df = pd.read_csv(CSV_FILE_NAME)
            if "Time (s)" in df.columns and "IR Value filtered" in df.columns:
                df = df.drop_duplicates(subset=["Time (s)"]).sort_values(by=["Time (s)"])
                times = np.array(df["Time (s)"])
                ir_signal = np.array(df["IR Value filtered"])
               
                nonlinear_results = []

                optimal_delay, amivalue = estimate_delay_ami(ir_signal, max_lag=50)
                fnn_vals = calculate_fnn(ir_signal, delay=optimal_delay, max_dim=10)
                fnn = plot_fnn(fnn_vals)
                embedding_dimension = next((d + 1 for d, val in enumerate(fnn_vals) if val < 1), 3)
                phase_space = reconstruct_phase_space(ir_signal, optimal_delay, embedding_dimension)
                lyap = calculate_lyapunov_exponent(phase_space)
                reconstruction_error = calculate_attractor_reconstruction_error(phase_space, delay=optimal_delay)
                dfa = calculate_dfa(ir_signal)
                nonlinear_results.append([amivalue, fnn, lyap, reconstruction_error, dfa])
                nonlinear_df = pd.DataFrame(nonlinear_results, columns=["AMI", "FNN", "Lyapunov", "ARE", "DFA"])
                nonlinear_df.to_csv(NONLINEAR_FEATURE_FILE_NAME, index=False)
                print("‚úÖ ƒê√£ l∆∞u c√°c ƒë·∫∑c tr∆∞ng v√†o 'nonlinear_results.csv'")
                time.sleep(10)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file '{CSV_FILE_NAME}'")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói kh√¥ng mong mu·ªën: {e}")

def predic_with_model():
    while True:
        print("‚è≥ [Prediction] ƒêang ƒë·ª£i 60 gi√¢y tr∆∞·ªõc khi ch·∫°y d·ª± ƒëo√°n...")
        time.sleep(70)
        try: 
            df_time = pd.read_csv(TIME_FEATURE_FILE_NAME)
            df_wavelet = pd.read_csv(WAVELET_FEATURE_FILE_NAME)
            df_nonlinear = pd.read_csv(NONLINEAR_FEATURE_FILE_NAME)
            df_frequency = pd.read_csv(FREQUENCY_FEATURE_FILE_NAME)
            final_results = []
            if df_time.empty or df_wavelet.empty or df_nonlinear.empty or df_frequency.empty:
                print("‚ö†Ô∏è M·ªôt trong c√°c file ƒë·∫∑c tr∆∞ng b·ªã r·ªóng, b·ªè qua l·∫ßn n√†y.")
                continue

            model_time = joblib.load(MODEL_TIME)
            model_wavelet = joblib.load(MODEL_WAVELET)
            model_nonlinear = joblib.load(MODEL_NONLINEAR)
            model_frequency = joblib.load(MODEL_FREQUENCY)

            pred_time = model_time.predict(df_time)[0]
            pred_wavelet = model_wavelet.predict(df_wavelet)[0]
            pred_nonlinear = model_nonlinear.predict(df_nonlinear)[0]
            pred_frequency = model_frequency.predict(df_frequency)[0]

            weight_sum = pred_time*acc_time + pred_nonlinear*acc_nonlinear + pred_wavelet*acc_wavelet + pred_frequency*acc_fourier
            threshold = (acc_wavelet + acc_nonlinear + acc_time +acc_fourier)/2

            final_pred = (weight_sum >= threshold).astype(int)

            final_results.append([pred_time, pred_wavelet, pred_nonlinear, pred_frequency, final_pred])

          
            final_df = pd.DataFrame(final_results, columns=["Prediction Time", "Prediction Wavelet", "Prediction Nonlinear", "Prediction Fourier", "Final Prediction"])
            final_df.to_csv(OUTPUT_FILE, index=False)

            print(f"‚úÖ [Prediction] D·ª± ƒëo√°n: Time = {pred_time:.2f}, Wavelet = {pred_wavelet:.2f}, Nonlinear = {pred_nonlinear:.2f}, Fourier = {pred_frequency:.2f}, Final={final_pred:.2f}")
            time.sleep(5)


        except Exception as e:
            print(f"‚ùå [Prediction] L·ªói d·ª± ƒëo√°n: {e}")



# Kh·ªüi ch·∫°y song song
t1 = threading.Thread(target=serial_reader)
t2 = threading.Thread(target=feature_time_extractor)
t3 = threading.Thread(target=fourier_feature_extractor)
t4 = threading.Thread(target=wavelet_feature_extractor)
t5 = threading.Thread(target=run_nonlinear_analysis)
t6 = threading.Thread(target=predic_with_model)


t1.start()
t2.start()
t3.start()
t4.start()
t5.start()
t6.start()

t1.join()
t2.join()
t3.join()
t4.join()
t5.join()
t6.join()
